{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP42Pdl42a7MNK1AeU4AVgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kekys778/avito_ds_2025/blob/main/training_part/training_part.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Тетрадка по обучению модели для контеста на авито***\n",
        "Предобработка текстов стандартная, на вкус и цвет, в зависимости от того, насколько фривольными тексты будут, которые мы собираемся инференсить"
      ],
      "metadata": {
        "id": "kEFE-LpRJiUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yesuureq-r3o"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_ID = 256\n",
        "VOCAB_SIZE = 257\n",
        "\n",
        "class RussianByteTokenizer:\n",
        "    \"\"\"Самописный токенайзер, который переводит текст в байты для байтовой модели\"\"\"\n",
        "    def __init__(self, pad_id=PAD_ID):\n",
        "        self.pad_id = pad_id\n",
        "        self._vocab_size = VOCAB_SIZE\n",
        "\n",
        "    def encode(self, text: str, max_length=384, truncation=True):\n",
        "        text = unicodedata.normalize('NFKC', text) #нормализуем Юникод\n",
        "        b = text.encode(\"utf-8\") #переводим в байты\n",
        "        if truncation:\n",
        "            b = b[:max_length] #трункуйтим до длины, модель обучалась на последорвательностях до 512\n",
        "        ids = list(b)\n",
        "        if len(ids) < max_length:\n",
        "            ids += [self.pad_id] * (max_length - len(ids))\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        valid = [i for i in ids if i != self.pad_id]\n",
        "        return bytes(valid).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    def pad_token_id(self):\n",
        "        return self.pad_id\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return self._vocab_size"
      ],
      "metadata": {
        "id": "J_amVgXu-0Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_space_insertion_data(text: str, removal_prob: float = 0.7) -> Tuple[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Удаляет пробелы с определенной вероятностью\n",
        "    \"\"\"\n",
        "    corrupted = \"\"\n",
        "    labels = []\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if char == ' ':\n",
        "            if random.random() < removal_prob:\n",
        "                if labels:\n",
        "                    labels[-1] = 1\n",
        "            else:\n",
        "                corrupted += char\n",
        "                labels.append(0)\n",
        "        else:\n",
        "            corrupted += char\n",
        "            if i < len(text) - 1 and text[i + 1] == ' ':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "\n",
        "    return corrupted, labels"
      ],
      "metadata": {
        "id": "mpcBmjOX_ICC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpaceDataset(Dataset):\n",
        "    def __init__(self, filepath: str, tokenizer: RussianByteTokenizer,\n",
        "                 seq_len: int = 1024, max_samples: int = None, removal_prob: float = 0.7):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.removal_prob = removal_prob\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            count = 0\n",
        "            for line in f:\n",
        "                self.samples.append(line)\n",
        "                count += 1\n",
        "\n",
        "                if count % 10000000 == 0:\n",
        "                    print(f\"Загружено {count} примеров\")\n",
        "                if count == 15_000_000:\n",
        "                  break\n",
        "\n",
        "        print(f\"Всего строк: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_text = self.samples[idx]\n",
        "\n",
        "        # убираем пробелы\n",
        "        corrupted_text, space_labels = create_space_insertion_data(\n",
        "            original_text, self.removal_prob\n",
        "        )\n",
        "\n",
        "        # наш токенизатор\n",
        "        input_ids = self.tokenizer.encode(corrupted_text, max_length=self.seq_len)\n",
        "\n",
        "        # наши лейблы с байтами\n",
        "        byte_labels = self.align_labels_with_bytes(corrupted_text, space_labels)\n",
        "\n",
        "        # паддим\n",
        "        if len(byte_labels) < self.seq_len:\n",
        "            byte_labels += [2] * (self.seq_len - len(byte_labels))\n",
        "        else:\n",
        "            byte_labels = byte_labels[:self.seq_len]\n",
        "\n",
        "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(byte_labels, dtype=torch.long)\n",
        "\n",
        "    def align_labels_with_bytes(self, text: str, char_labels: List[int]) -> List[int]:\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        bytes_text = text.encode('utf-8')\n",
        "        byte_labels = []\n",
        "        char_idx = 0\n",
        "\n",
        "        i = 0\n",
        "        while i < len(bytes_text) and char_idx < len(char_labels):\n",
        "            byte_val = bytes_text[i]\n",
        "\n",
        "            if byte_val < 128:\n",
        "                byte_labels.append(char_labels[char_idx])\n",
        "                char_idx += 1\n",
        "                i += 1\n",
        "            else:\n",
        "                if (byte_val & 0xE0) == 0xC0:\n",
        "                    char_bytes = 2\n",
        "                elif (byte_val & 0xF0) == 0xE0:\n",
        "                    char_bytes = 3\n",
        "                elif (byte_val & 0xF8) == 0xF0:\n",
        "                    char_bytes = 4\n",
        "                else:\n",
        "                    char_bytes = 1\n",
        "\n",
        "                label = char_labels[char_idx] if char_idx < len(char_labels) else 0\n",
        "                for j in range(char_bytes):\n",
        "                    if i + j < len(bytes_text):\n",
        "                        byte_labels.append(label)\n",
        "\n",
        "                char_idx += 1\n",
        "                i += char_bytes\n",
        "\n",
        "        return byte_labels"
      ],
      "metadata": {
        "id": "jsJVnarBDdlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpaceInsertionModel(nn.Module):\n",
        "    \"\"\"Модель выдает вероятность пробела после каждого символа\"\"\"\n",
        "    def __init__(self, vocab_size: int = 257, hidden_dim: int = 384,\n",
        "                 num_layers: int = 6, num_heads: int = 6):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.byte_embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=PAD_ID)\n",
        "        self.pos_embedding = nn.Embedding(512, hidden_dim) #позиционные обучаемые эмбеддинги\n",
        "\n",
        "        # энкодер на трансформере (EO-архитектура)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim, #архтектура попроще, чем в статье\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # последний слой: классфикатор выдает вероятность пробела\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Embeddings\n",
        "        token_emb = self.byte_embedding(x)\n",
        "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        pos_emb = self.pos_embedding(pos_ids)\n",
        "\n",
        "        emb = token_emb + pos_emb\n",
        "        emb = self.dropout(emb)\n",
        "\n",
        "        padding_mask = (x == PAD_ID)\n",
        "\n",
        "        encoded = self.encoder(emb, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        logits = self.classifier(encoded)  # (batch, seq, 2)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "nstieK9kFTdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_space_positions(text: str) -> set:\n",
        "    \"\"\"Возвращает индексы символов, после которых стоит пробел\"\"\"\n",
        "    return {i for i, ch in enumerate(text) if ch == \" \"}\n",
        "\n",
        "\n",
        "def f1_score_spaces(true_texts, pred_texts) -> float:\n",
        "    \"\"\"Вычисляет средний ф1\"\"\"\n",
        "    f1_scores = []\n",
        "\n",
        "    for true, pred in zip(true_texts, pred_texts):\n",
        "        true_spaces = get_space_positions(true)\n",
        "        pred_spaces = get_space_positions(pred)\n",
        "\n",
        "        if not true_spaces and not pred_spaces:\n",
        "            f1_scores.append(1.0)\n",
        "            continue\n",
        "        if not pred_spaces:\n",
        "            f1_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        tp = len(true_spaces & pred_spaces)\n",
        "        precision = tp / len(pred_spaces) if pred_spaces else 0.0\n",
        "        recall = tp / len(true_spaces) if true_spaces else 0.0\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return sum(f1_scores) / len(f1_scores)"
      ],
      "metadata": {
        "id": "uq6HCKmUFaM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_spaces_silent(model, text: str, tokenizer: RussianByteTokenizer, device) -> str:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    input_text = text.replace(' ', '')\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, max_length=384)\n",
        "    input_tensor = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "        space_probs = probabilities[0, :, 1]\n",
        "        predictions = (space_probs > 0.5).cpu().numpy()\n",
        "\n",
        "    result = \"\"\n",
        "    input_bytes = input_text.encode('utf-8')\n",
        "\n",
        "    byte_idx = 0\n",
        "    for char in input_text:\n",
        "        result += char\n",
        "\n",
        "        char_bytes = len(char.encode('utf-8'))\n",
        "        should_add_space = False\n",
        "\n",
        "        for i in range(char_bytes):\n",
        "            if byte_idx + i < len(predictions) and predictions[byte_idx + i]:\n",
        "                should_add_space = True\n",
        "                break\n",
        "\n",
        "        if should_add_space and result[-1] != ' ':\n",
        "            result += ' '\n",
        "\n",
        "        byte_idx += char_bytes\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Rhb4vAPUF7bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_f1(model, dataset, tokenizer, device, num_samples=1000, print_examples=3):\n",
        "    \"\"\"Вычислить ф1 и выводит примеры\"\"\"\n",
        "    model.eval()\n",
        "    true_texts, pred_texts = [], []\n",
        "\n",
        "    print(f\"\\n=== оценка на  {min(num_samples, len(dataset))} примерах ===\")\n",
        "\n",
        "    for i in tqdm(range(min(num_samples, len(dataset)))):\n",
        "        original_text = dataset.samples[i]\n",
        "        corrupted_text, _ = create_space_insertion_data(original_text, removal_prob=0.7)\n",
        "\n",
        "        pred_text = insert_spaces_silent(model, corrupted_text, tokenizer, device)\n",
        "\n",
        "        true_texts.append(original_text)\n",
        "        pred_texts.append(pred_text)\n",
        "\n",
        "        if i < print_examples:\n",
        "            print(f\"\\nПример {i+1}:\")\n",
        "            print(f\"Оригинал: '{original_text}'\")\n",
        "            print(f\"Испорчено:'{corrupted_text.replace(' ', '')}'\")\n",
        "            print(f\"Предскапзано:'{pred_text}'\")\n",
        "            print(f\"А совпадает ли?: {original_text == pred_text}\")\n",
        "\n",
        "    f1 = f1_score_spaces(true_texts, pred_texts)\n",
        "    print(f\"\\nф1: {f1:.4f}\")\n",
        "    print(\"=\" * 50)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "96faMRzsFjXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# наша обертка для трейна\n",
        "\n",
        "import os\n",
        "def train_space_insertion_model(model, train_loader, device, save_dir, epochs=2):\n",
        "    os.makedirs(save_dir, exist_ok=True) #для чекпоинтов\n",
        "\n",
        "    best_space_acc = 0.0\n",
        "    start_epoch = 0\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.95)\n",
        "    )\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(\n",
        "        ignore_index=2,\n",
        "        weight=torch.tensor([1.0, 3.0]).to(device)\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=3e-4,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "    accumulation_steps = 4\n",
        "\n",
        "\n",
        "    checkpoint_path = os.path.join(save_dir, \"checkpoint_epoch_0_batch_75000.pth\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_space_acc = checkpoint.get('best_space_acc', 0.0)\n",
        "        print(f\" Чекпоинт {start_epoch}, лучший скор: {best_space_acc:.4f}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_predictions = 0\n",
        "        space_predictions = 0\n",
        "        space_correct = 0\n",
        "\n",
        "        for batch_idx, (input_ids, labels) in enumerate(train_loader):\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_ids)\n",
        "                    mask = (labels != 2)\n",
        "\n",
        "                    if mask.sum() > 0:\n",
        "                        loss = criterion(logits[mask], labels[mask]) / accumulation_steps\n",
        "\n",
        "                scaler.scale(loss).backward() #используем скейлер для ускорения обучения\n",
        "            else:\n",
        "                logits = model(input_ids)\n",
        "                mask = (labels != 2)\n",
        "\n",
        "                if mask.sum() > 0:\n",
        "                    loss = criterion(logits[mask], labels[mask]) / accumulation_steps\n",
        "                    loss.backward()\n",
        "\n",
        "            # использузуем накопление градиента для имитации большего батчсайза\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                if scaler:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            # лосс\n",
        "            if mask.sum() > 0:\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    correct = (predictions[mask] == labels[mask]).sum().item()\n",
        "                    total_correct += correct\n",
        "                    total_predictions += mask.sum().item()\n",
        "\n",
        "                    # проверяем нашу аккураси\n",
        "                    space_mask = (labels == 1) & mask\n",
        "                    if space_mask.sum() > 0:\n",
        "                        space_pred_correct = (predictions[space_mask] == labels[space_mask]).sum().item()\n",
        "                        space_correct += space_pred_correct\n",
        "                        space_predictions += space_mask.sum().item()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "                current_acc = total_correct / max(total_predictions, 1)\n",
        "                space_acc = space_correct / max(space_predictions, 1)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, \"\n",
        "                      f\"Loss: {loss.item()*accumulation_steps:.4f}, \"\n",
        "                      f\"Overall Acc: {current_acc:.4f}, \"\n",
        "                      f\"Space Acc: {space_acc:.4f}\")\n",
        "\n",
        "            if batch_idx % 5000 == 0 and batch_idx > 0:\n",
        "              checkpoint = {\n",
        "              'epoch': epoch,\n",
        "              'batch': batch_idx,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'best_space_acc': best_space_acc,\n",
        "              'loss': loss.item() if mask.sum() > 0 else 0.0\n",
        "                }\n",
        "              torch.save(checkpoint, os.path.join(save_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth\"))\n",
        "              print(f\"Checkpoint saved at epoch {epoch}, batch {batch_idx}\")\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = total_correct / max(total_predictions, 1)\n",
        "        space_accuracy = space_correct / max(space_predictions, 1)\n",
        "        val_f1 = evaluate_f1(model, train_loader.dataset, tokenizer, device, num_samples=500)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} |\"\n",
        "              f\"Acc: {accuracy:.4f} | Space Insertion Acc: {space_accuracy:.4f} | Validation F1: {val_f1:.4f}\")\n",
        "        if space_accuracy > best_space_acc:\n",
        "            best_space_acc = space_accuracy\n",
        "            best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_space_acc': best_space_acc,\n",
        "                'accuracy': accuracy,\n",
        "                'loss': avg_loss\n",
        "            }, best_model_path)\n",
        "            print(f\"New best model saved! Space accuracy: {space_accuracy:.4f}\")\n",
        "\n",
        "        # сохраняем чекпоин\n",
        "        latest_checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_space_acc': best_space_acc,\n",
        "            'accuracy': accuracy,\n",
        "            'loss': avg_loss\n",
        "        }\n",
        "        torch.save(latest_checkpoint, os.path.join(save_dir, \"latest_checkpoint.pth\"))\n",
        "\n",
        "        epoch_checkpoint_path = os.path.join(save_dir, f\"epoch_{epoch+1}_checkpoint.pth\")\n",
        "        torch.save(latest_checkpoint, epoch_checkpoint_path)\n",
        "        print(f\"Epoch {epoch+1} checkpoint saved\")"
      ],
      "metadata": {
        "id": "_0Gl1zD1GEwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Zu2UUANLIbQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RussianByteTokenizer()\n",
        "dataset = SpaceDataset(\n",
        "        # '/kaggle/input/wikicleaned/wiki_all.txt',\n",
        "        tokenizer,\n",
        "        seq_len=384,\n",
        "        max_samples=0, #max_samples,\n",
        "        removal_prob=0.7  # Remove 70% of spaces for training\n",
        "    )"
      ],
      "metadata": {
        "id": "1RtliZQlITSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if device.type == 'cuda' else False\n",
        "    )"
      ],
      "metadata": {
        "id": "MtQc_jUBInhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpaceInsertionModel(\n",
        "        vocab_size=tokenizer.vocab_size(),\n",
        "        hidden_dim=384,\n",
        "        num_layers=6,\n",
        "        num_heads=6\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "dfOLZkwiIrrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_space_insertion_model(model, train_loader, device, '/kaggle/working/', epochs=3)"
      ],
      "metadata": {
        "id": "UIPBiA62I48U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}